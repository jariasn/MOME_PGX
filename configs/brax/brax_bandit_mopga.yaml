hydra:
    run:
        dir: ./results/${env_name}/${algo_name}/${now:%Y-%m-%d_%H%M%S}_${seed}

# Env config
seed: 42
env_name: walker2d_multi
algo_name: bandit_mopga
fixed_init_state: True
episode_length: 1000

# MOO parameters
num_objective_functions: 2
pareto_front_max_length: 50
bias_sampling: False

# Initialisation parameters
num_evaluations: 1024000
num_init_cvt_samples: 50000 
num_centroids: 128 

# Brax parameters
num_descriptor_dimensions: 2
policy_hidden_layer_sizes: [64, 64] 

# Min and max val of BDs
minval: 0. 
maxval: 1.

# Emitter parameters
iso_sigma: 0.005 
line_sigma: 0.05 
total_batch_size: 256
bandit_scaling_param: 1.
dynamic_window_size: 50
many_emitters: False

# Logging parameters
metrics_log_period: 50
plot_repertoire_period: 400
checkpoint_period: 500
save_checkpoint_visualisations: False
save_final_visualisations: True
num_save_visualisations: 5

# TD3 params
replay_buffer_size: 1000000 
critic_hidden_layer_size: [256, 256] 
critic_learning_rate: 0.0003
greedy_learning_rate: 0.0003
policy_learning_rate: 0.001
noise_clip: 0.5 
policy_noise: 0.2 
discount: 0.99 
reward_scaling: [1.0, 1.0]  
transitions_batch_size: 256 
soft_tau_update: 0.005 
policy_delay: 2
num_critic_training_steps: 300 
num_pg_training_steps: 100 


